---
title: "intro_to_codec"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{intro_to_codec}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# CoDEC

The **Co**mmunity **D**ata **E**xplorer for **C**incinnati (CoDEC) is a data resource composed of equitable, community-level data.

CoDEC defines common standards for harmonizing, storing, and accessing community-level data.

CoDEC is a software package for R that provides tools to describe, curate, and check tabular data resources against CoDEC standards. The result is a harmonized, community-level tabular data package that is openly available and accompanied by a (1) a richly documented data catalog, (2) a web-based interface for exploring and learning from data, and (3) an API for accessing data at scale and on demand.

Like its [namesake](https://en.wikipedia.org/wiki/Codec), CoDEC encodes data streams about the communities in which we live into a common format so that it can be decoded into different community-level geographies:

Cool figure about codec here... (include example outputs of data at zipcodes in 2010, census tracts in 2020, neighborhood, etc...)

We have initialized CoDEC with extant community-level data from ... (hh_acs_measures, etc) ... .

Data about the communities in which we live come in different spatiotemporal resolutions and extents and often are not designed with the specific goal of integrating with other data. Furthermore, these data often contain private, individual-level information, when we are only interested in community-level (e.g. neighborhood) measures. Achieving data harmonization upstream of storage allows for contribution of aggregate, community-level data without disclosing invididual-level data when sharing across organizations. By creating specifications, data is assembled transparently and reproducibly.  Data structure is automatically validated using the specifications. This saves time and resources, leading to increased efficiency and accelerated innovation, but also boosts trustworthiness.

Although this data is all extant, it shouldn't make us feel like we need this specialized expertise to curate and use data.

A single point for data consumption provides ownership of the process of harmonizing data and integrates well with data governance, but most importantly, can provide data to be consumed in multiple ways (e.g. dashboard, tabular data file, API)

## Equitable Population Data

### The White House's Equitable Data Working Group:^[https://www.whitehouse.gov/briefing-room/statements-releases/2022/04/22/fact-sheet-biden-harris-administration-releases-recommendations-for-advancing-use-of-equitable-data/]

> Equitable data are those that allow for rigorous assessment of the extent to which government programs and policies yield consistently fair, just, and impartial treatment of all individuals. Equitable data illuminate opportunities for targeted actions that will result in demonstrably improved outcomes for underserved communities.

### Denice Ross, U.S. Chief Data Scientist:^[https://www.fedscoop.com/us-chief-data-scientist-interview/]

> "Open data is necessary and *not* sufficient to drive the type of action that we need to create a more equitable society."  The "next generation of open data" is disaggregated data.

### One of the three "*Priority Uses for Equitable Data*":

- **generating disaggregated statistical estimates to characterize experiences** of historically underserved groups using survey data

### One part of their "*Equitable Data Vision*":

- **make disaggregated data the norm while protecting privacy**

"... must be intentional about when data are collected and shared, as well as how data are protected so as not to exacerbate the vulnerability of members of underserved communities, many of whom face the heightened risk of harm if their privacy is not protected."


## [Frictionless](https://frictionlessdata.io/) [Standards](https://specs.frictionlessdata.io/)

Developed by the [Open Knowledge Foundation](https://okfn.org/) 

- A set of patterns for describing data, including datasets (Data Package), files (Data Resource), and tables (Table Schema)
- A Data Package is a simple container format used to describe and package a collection of data and metadata, including schemas
- These metadata are contained in a specific file (separate from the data file), usually written in JSON or YAML, that describes something specific to each Frictionless Standard:
  - [Table Schema](https://specs.frictionlessdata.io/table-schema/): describes a tabular file by providing its dimension, field data types, relations, and constraints
  - [Data Resource](https://specs.frictionlessdata.io/data-resource/): describes an *exact* tabular file providing a path to the file and details like title, description, and others
  - [Tabular Data Resource](https://specs.frictionlessdata.io/tabular-data-resource/) = Data Resource + Table Schema
  - [Data Package](https://specs.frictionlessdata.io/data-package/) & [Tabular Data Package](https://specs.frictionlessdata.io/tabular-data-package/): describes a *collection* of tabular files providing data resource information from above along with general information about the package itself, a license, authors, and other metadata
  - [Data Package Identifier](https://specs.frictionlessdata.io/data-package-identifier/) (supports URL file & folder, github url; use as argument when specifying data)
  - [CSV dialect](https://specs.frictionlessdata.io/csv-dialect/)
